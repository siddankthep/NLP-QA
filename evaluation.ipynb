{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775212a7-28e5-47d7-a6c8-56c41e148a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa86799d8d854726b1f974b2a6b85f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c363f9-4dc4-4b86-aacd-3f8b4369c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9fb4291-57bb-4489-b96e-96346afc329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "621635f0-9796-441d-92dc-ad2632736c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, TextStreamer\n",
    "def generate(prompt, max_new_tokens=1024):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # temperature=0.4,\n",
    "            # top_p=0.95,\n",
    "            # top_k=20,\n",
    "            # bos_token_id=tokenizer.bos_token_id,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # eos_token_id=0, # for open-end generation.\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=False,\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        # streamer = None\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            # stopping_criteria=stopping_criteria,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, len(input_ids[0]):]\n",
    "    output = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    output = output.split(tokenizer.eos_token)[0]\n",
    "    return output.strip()\n",
    "\n",
    "def generate_batch(prompts, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generates text outputs for a batch of input prompts.\n",
    "\n",
    "    Args:\n",
    "        prompts: A list of strings representing the input prompts.\n",
    "        max_new_tokens: The maximum number of tokens to generate for each prompt.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the generated text outputs.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompts in a batch\n",
    "    input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generation_config = GenerationConfig(\n",
    "            repetition_penalty=1.0,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=False, \n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "        generated = model.generate(\n",
    "            inputs=input_ids,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "\n",
    "    # Decode the generated outputs\n",
    "    gen_tokens = generated[\"sequences\"].cpu()[:, input_ids.shape[1]:]  # Remove input prompt tokens\n",
    "    outputs = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)  \n",
    "    outputs = [output.strip() for output in outputs]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8d7738c-f66d-4244-9b72-5179a9182134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "answers_path = \"answers_prompts.jsonl\"\n",
    "questions_path = \"questions_prompts.jsonl\"\n",
    "with open(answers_path, 'r') as file:\n",
    "    answers_prompts = []\n",
    "    for line in file:\n",
    "        answers_prompts.append(json.loads(line))\n",
    "with open(questions_path, 'r') as file:\n",
    "    questions_prompts = []\n",
    "    for line in file:\n",
    "        questions_prompts.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ba2abf8-7127-4333-ba62-0df5193870e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "answers_prompts_batch = [answers_prompts[i:i+batch_size] for i in range(0, len(answers_prompts), batch_size)]\n",
    "questions_prompts_batch = [questions_prompts[i:i+batch_size] for i in range(0, len(questions_prompts), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a0878cd-1fe9-461b-9fca-05ecc1eb238d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers_prompts_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "418fefdf-3109-4012-b705-d145dc541401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb647baa1334a4091d615fceef355ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "for batch in tqdm(answers_prompts_batch[:250]):\n",
    "    prompts = [example[\"prompt\"] for example in batch]\n",
    "    prompts = [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts]\n",
    "    responses = generate_batch(prompts)\n",
    "    for example, response in zip(batch, responses):\n",
    "        # print(response)\n",
    "        example.update({\"evaluation\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e86734ea-58d4-4654-841b-da88d73de66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222ad3ca23794cdb9475bf2a3a695e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in tqdm(questions_prompts_batch[:250]):\n",
    "    prompts = [example[\"prompt\"] for example in batch]\n",
    "    prompts = [PROMPT_TEMPLATE.format(prompt=prompt) for prompt in prompts]\n",
    "    responses = generate_batch(prompts)\n",
    "    for example, response in zip(batch, responses):\n",
    "        # print(response)\n",
    "        example.update({\"evaluation\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df20964f-0331-4323-b9f5-7d5893af167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_questions = [x[0] for x in questions_prompts_batch[:250]]\n",
    "save_answers = [x[0] for x in answers_prompts_batch[:250]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9ca7c2d-e6f0-47e4-9308-539e957aec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for example in save_questions:\n",
    "    scores = json.loads(example[\"evaluation\"])\n",
    "    example.update({\"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ba0a170-d9d7-48b1-8890-b3547278aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in save_answers:\n",
    "    try:\n",
    "        scores = json.loads(example[\"evaluation\"])\n",
    "    except:\n",
    "        scores = None\n",
    "    example.update({\"scores\": scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edef1f39-4ac4-4267-a800-2ce71125a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to save_questions.jsonl\n"
     ]
    }
   ],
   "source": [
    "filename = \"save_questions.jsonl\"\n",
    "with open(filename, 'w') as file:\n",
    "    for item in save_questions:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5dbac76d-23fd-425f-9ade-d5f38d606986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to save_answers.jsonl\n"
     ]
    }
   ],
   "source": [
    "filename = \"save_answers.jsonl\"\n",
    "with open(filename, 'w') as file:\n",
    "    for item in save_answers:\n",
    "        json.dump(item, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
